.env
snowflake-cortex-sourceSuzie. Hello. Hey, Cortana. Hey, Cortana. Oh, Sunday. Wish. 02AP 8991. Yes. Battery recently changed because atmosphere. Whatsapp number Whatsapp number. 8767. 435. 098. Yes. Ha ha ha ha. Thank you. Hello. Hello. Hello. OK OK. Peter, thank you. Hello. Hello. Hello. Hello. Hello. Yeah. Yeah. It is 800. Yeah, sure. Thank you. Thank you. Thank you. I. Yeah. I. I. Now. Do you do? Hey. Bed. Yeah, yeah. Hey, Cortana. Hey, Cortana. Hello. Hello. Hey, Cortana. Hey, Cortana. Hey, Cortana. I. I. Hey, welcome. Hey, Cortana. OK. Hey, hi, Prashanth. Hey, hi, Prashant. Prakash. Sorry. Hey, hi, Prakash. Good afternoon. Hi thank you so much for taking this discussion. So this discussion is regarding an Azure database engineer wherein you know we are looking for people having good skills not Azure database engineer but Azure database AI and ML engineer. So here we are looking for people having good skills on database of course but the end to end mall OPS pipeline he should know all this end to end, how to integrate the monitoring the deployment. The politician aspects as well and also have good knowledge on coding like Python and SQL. So these are the kind of skills we are looking for. So my name is Ritish and I lead here for MLOPS as well as LLM OPS. So yeah, that's all about myself and the rule. Why don't you go ahead and introduce yourself Prakash and then we can get started. OK. OK. OK OK. OK. OK. OK, OK, OK. So was this a generative project or was this just document extraction? Great. OK. OK OK. OK, OK. That's good to know, Prakash. So, any experience on Databricks? OK. OK. Got it. Got it. OK OK got it. So why I was asking is because there will be some sequel as well so you know data breaks and SQL go hands on hand. So I'll be posting you a SQL question. So before that can you please share your screen. You need to hover to some other tab because this question would ask you to write a SQL query. Can you please first share your speed? Yeah, yeah. At the bottom there is a share screen option. Yeah, right. So can you please open a new tab? And type 11 compiler SQL. SQL. Yeah, yeah, yeah. Open that. Yeah, just hold on, please. It loads. It will take some time to load, just go up. For you it is taking longer than expected. Can you? Yeah, now it has come. Can you please delete everything from here? Yeah now go to interview page. I have posted you the question. Yeah, copy the entire question. Don't miss out on anything or else it will give you an error. No, no, you are not copying the entire question. Copy the entire question when I mean entire question from the top to the bottom. Yeah, yeah, yeah. Copy. Yeah, I think you have copied more than what was intended. So from 48 to 64 please remove it. Yeah, yeah. And Justice in line number 47. Go to line number 47 please yeah and just type select star from students. Just type. I just want to check whether the ID is working presently or not. Sometime it gives you an error select start from students. Yeah and put a semi colon yeah and run the code run run option will be at the top. Yeah so this code runs. Yeah, so now you have 8 minutes to solve this question. OK please go through the question. The question is in line number one. The pattern of output is in after that and then few tables have been created for you and some data has also been inserted for you. You have to answer the question by writing an effective SQL code. Yes. 6 more minutes is remaining OK. Three and half more minute is remaining OK. Three and half minutes. Final one minute OK. Sorry. No I would want. I would want the marks for all the students, not only one student. See the how the example is set up in. In the initial how it is set up, Alice's marks has to be there, Bobby marks has to be there, charlies and even Charlies. 3rd highest marks is 78, but you have printed 88, correct? OK. No worries, I'll give you some question on Python. Now just a second. You can stop sharing your screen now. And I'm posting your question. Kindly solve this question. No, no no no you need to solve here. From here, yeah, from line number 4 maybe. Yeah, time is 6 minutes. OK. Please start, lady, you have already exhausted one minute. Write a Python function OK. 3 more minutes is remaining OK. Execute that function right? You have just defined the function. You have to execute by calling the function. OK no problem no problem. Just copy copy the input date input date from the question from the question, place it in line #15. No worries, no need to worry. Input tip from the question and place it in line #4 D. Yeah, now here and yeah, and. Yeah, and now print print the swap. Return or friend to submitting this. So now you have a problem in your phone. 2 more minutes is remaining OK. One more minute, last minute. Time is up for this question. I am posting another question Mr so here you need to solve only question number two you need to solve any other question OK time is again five minutes you can copy the entire question. And print the DF. And check time is 5 minutes. Yeah, only question number two is required. You can skip question number one answering, but 4 and 1/2 more minutes is remaining. You can copy the entire question like from answer to print DF you can copy and check how this DF looks like. That is normally how I run the code and check how this code works. Run the code. OK import numpy and import PD you have forgotten. Just copy and paste this to precious. Copy from the question only, it will save you time. Yeah, now check and run the photo once. Yeah, now please answer question number 2 in 4 minutes. Understood the question right. Yeah. One and a half more minutes. OK. That is done. So you have taken median mean standard deviation. The lower is mean three standard deviation of this 3 standard deviation. XS mini OK. That is fine, sure. Let me ask you on data bricks now. Can you please read through the question and then answer? Which alternate tool would you need? Would you want? Would you take for explanation? Huh. OK. OK. OK, OK. So in mill flow, how would you do the tracking and deployment and governance? Using mill flow, how would you do model tracking, model deployment and model coordinates? The first question. So, so, so enable flow. How would you handle the versioning of the model? How would you ensure that the best model is used for deployment? How would you do that? How would you do hyperparameter tuning by? Using the best practices or the best features of mill code? And what about the governance part? The governance. OK so if you have not worked with the data breaks then in Azure how would you handle the governance part of model? Because in database it is very easy actually. But since you have not used database for mill model governance how would you handle that in Azure since your? Governance is like who has validated this model? Does this model. Ensure that drifts and all everything is captured or not. So all these steps comes under governance. OK. Yeah, yeah. Please go ahead. OK OK. Now second question, since you are not aware with the Delta lake and the mill database concept, can you tell me like how would you handle the features which are incoming? How would you handle the changes in features over time using any cloud provider of your choice? Yeah, added or deleted or some changes happens because model returning happens with new data sets. Model retraining may happen with completely new columns. How would you handle them? How would you version them? What kind of feature store is there in Azure or any other cloud service provider that you have worked on? How would you manage those features? Tools. Is DBC sufficient enough for handling the features overtime? OK and can we move to the third part? How would you OK and how would you handle Github Actions with the with your ever changing code basis? OK. OK. OK OK sure. OK OK. Fine. I'm giving you a question on data warehousing and model retraining. Please look into this use case and then you can answer. So whenever there is change in data and the model returning subsequently there are updates in the metadata of the table as well. Many tables may change their structures, not structures, the data content and all. Not only a single table. Because their metadata has to be updated, the historical information has to be updated. So how would you handle those stable and the changes which does not affect your machine learning output means which affects your machine learning output, but very indirectly. Everything is at the back end like machine learning. Output is maybe what the consumer gets, but. At that means and the behind the scenes the tables are schemas of and the metadata get updated. So how would you handle them? Second is how would you handle the data preprocessing for the ETL. So whenever your data comes in 1st the preprocessing would happen, then only the model would be trained with the refined or the refreshed data #2 and #3 how would you handle or which kind of framework would you use to? Handle the model for retraining after the EPL process has happened. How would you handle the retrieving of the model in that warehouse? So first start with the table schemas and views, which are the metadata and which needs to be ready for the update. OK and how would you handle a connect the job to? Which service would you connect the job to? So in Azure Devops you have jobs, I agree Azure Pipelines, but how would you connect the job to? Which service in Azure would you connect the job to? OK now can we move to the data preprocessing using ETL? Which kind of service would you use for preprocessing the data? Because when when I'm talking about data preprocessing for ML training, it involves a lot of data maybe sometime 3540 GB of data for a one single model training. So how would you handle that much of data? What kind of ETL service would you use? Yeah. OK and can we move to the model retraining in a warehouse. Suppose I give you a data breaks or say a snowflake kind of an environment or a data prop kind of an environment. How would you retrain the model considering your data is lying on so and so source which you have not discussed with me but how would you take that data in just into your model? What will be your approach in ingesting the data while your model is? Starting to get retrained because it's a lot of data. How would you handle the choking of the models returning factor? Because when there is a lot of data your model can you know gets trained through maybe a batch of data. So how would you handle the choking factors? No, not preprocessing. Reversing is done. You said that you have not worked, but how would you handle the model retraining? Suppose it has been done. Now how would you do? Would you do real time? Would you do batch? If it is real time model retraining, how would you handle that? If you are going for a batch wise retraining, how would you handle that after your preprocessing here is complete? OK, I am posting you a new question. Please check this. But then you can do it using the flame docker, right? Why would you use Docker compose? OK no issues at all. Can you please answer this question on the two deployments that I have? Mentioned. Are you sure we need to maintain two environment in a bluegrind deployment? Are you sure? When you may, when you say to environment, what is your definition of environment? We can only have one cluster for a bluegrained deployment. Two clusters are not actually required. OK. No worries, let me ask you some other question. Can you please answer this question on Data Warehouse? OK OK. And what about OLAPI? OK OK. OK. OK got it. So for machine learning model training which system would you use? For machine learning model inferencing which system would use? I mean I mean if you were given to do a machine learning model training, would you choose an OLAP system or an OLTP system? Similarly if you were given to use model predict? Or model inferencing, which type of system would you use for that system? Or OLTP system? So for model dot fit in very layman language? For model fit which system would you choose? OLAP or OLTP for model predict? Which system would you choose? OLAP or Olivier? And why? OK. OK. And for and for model predict. OK. OK got it. Yeah yeah that is fine that is fine. Can you please answer this how how this 4 databases aids and Mlops engineer for their day to day activity how they are used? Can you please tell me? OK OK. OK. OK. That is fine my. Last question to you is on slowly changing dimensions. So can you please tell me what do you mean or what is your understanding of stories in dimensions? It is related to features too. It is used in future stores. How, how? How would you handle them? So slowly changing dimensions. OK, no worries. Thank you Prakash for your time. The Concerned HR team will get back to you over the response. OK. Thank you so much. Have a great day. Bye. You can do. Hello. Play Mujhe. Thank you. Two Mamas spoiled act. 